\documentclass{beamer}

% \usetheme{Singapore}
% \usetheme{Malmoe}
\usetheme{Warsaw}

\usepackage[utf8]{inputenc}
\usepackage[russian]{babel}
\usepackage{cmap}
\usepackage{mathrsfs}
\usepackage{changepage}

% \useoutertheme{split}
\useoutertheme{shadow}
\usefonttheme{professionalfonts}
\usepackage{graphicx}
\usepackage{psfrag}
\beamertemplatenavigationsymbolsempty 
\DeclareMathOperator{\sign}{sign}

\setbeamertemplate{footline}[frame number]

\author{Павел Филонов \\ \href{mailto:filonovpv@gmail.com}{filonovpv@gmail.com}}
\title{Введение в машинное обучение}
\subtitle{Основные понятия и определения}

\begin{document}
\begin{frame}[plain]
    \titlepage
\end{frame}
\begin{frame}[plain]{Содержание}
  \tableofcontents
\end{frame}
\section{Основные понятия и обозначения}
\subsection{Постановка задачи машинного обучения}
\begin{frame}{Постановка задачи машинного обучения}
Пусть
    \begin{itemize}
        \item $X$ --- множество объектов (objects);
        \item $Y$ --- множество ответов (targets);
        \item $y: X \rightarrow Y$ --- неизвестная зависимость (target function).
    \end{itemize}
    {\bf Дано:}
    \begin{itemize}
        \item $\{x_1, \dots, x_l\} \subset X$ --- обучающая выборка (training sample);
        \item $y_i = y(x_i),~ i=1,\dots, \ell$ --- известные ответы (known targets).
    \end{itemize}
    {\bf Найти:}
    $a: X \rightarrow Y$ --- алгоритм, решающую функцию (decision function), приближающую $y$ на всём множестве $X$.


Весь курс машинного обучения--- это конкретизация:
    \begin{itemize}
        \item как задаются объекты и какими могут быть ответы;
        \item в каком смысле <<$a$ приближает $y$>>;
        \item как строить функцию $a$.
    \end{itemize}

% Приведенная задача --- представляет собой центральную проблему, которая красной нитью проходит через весь курс.
\end{frame}

\subsection{Представление объектов и ответов.}
\begin{frame}
$f_j: X \rightarrow D_j, j=1,\dots,n$ --- принаки объектов (features).

Типы признаков:
\begin{itemize}
    \item $D_j = \{0, 1\}$ --- бинарный признак;
    \item $|D_j| < \infty$ --- номинальный признак;
    \item $|D_j| < \infty, ~ D_j - \text{упорядочено}$ --- порядковый признак;
    \item $D_j = \mathbb{R}$ --- количественный признак.
\end{itemize}

Вектор $\left(f_1(x),\dots,f_n(x)\right)$ --- признаковое описание объекта $x$.

Матрица <<объекты-признаки>> (feature matrix)
$$
F = ||f_j(x_i)||_{\ell \times n} = \begin{pmatrix}
    f_1(x_1) & \cdots & f_n(x_1) \\
    \vdots   & \ddots &   \vdots \\
    f_1(x_\ell) & \cdots & f_n(x_\ell) \\
    \end{pmatrix}
$$
\end{frame}

\begin{frame}{Представление ответов. Типы задач}
    
    {\bf Задача классификации} (classification):
    \begin{itemize}
        \item $Y = \{-1, +1\}$ --- бинарная классификация;
        \item $Y = \{1, \dots, M\}$ --- на $M$ непересекающихся классов;
        \item $Y = \{0, 1\}^M$ --- на $M$ классов, которые могут пересекаться.
    \end{itemize}
    \vfill

    {\bf Задача восстановления регресии} (regression):
    \begin{itemize}
        \item $Y = \mathbb{R}$ --- одномерная;
        \item $Y = \mathbb{R}^m, ~ m > 1$ --- многомерная (multivariate).
    \end{itemize}
    \vfill

    {\bf Задача ранжирования} (ranking)
    \begin{itemize}
        \item $Y$ --- конечное упорядоченное множество.
    \end{itemize}
\end{frame}
\begin{frame}{Задача классификации цветков ириса [Фишер, 1936]}
    \footnotesize $n = 4$ количественный признака, $|Y| = 3$ класса, объем выборки $\ell = 150$.
    \includegraphics[width=\linewidth]{fig/iris.png}
\end{frame}
\begin{frame}{Задача оценки стоимости жилья в Бостоне}
    \begin{center}
        $n = 13, Y= \mathbb{R}, l = 506$
        \includegraphics[width=\linewidth]{fig/boston1.png}
        \\
        \includegraphics[width=\linewidth]{fig/boston2.png}
    \end{center} 
\end{frame}

\subsection{Модели и методы обучения}
\begin{frame}{Модель алгоритмов}
Модель (predictive model) --- парметрическое семейство функций
$$
    A = \{g(x, \theta) ~|~ \theta \in \Theta\},
$$
где $g : X \times \Theta \rightarrow Y$ --- фиксированная функция,\\
$\Theta$ --- множество допустимых значений параметра $\theta$.

{\bf Пример.}
Линейная модель с вектором параметров $\theta = (\theta_1,\dots,\theta_n), \Theta = \mathbb{R}^n$:
$$
    g(x, \theta) = \sum\limits_{j=1}^{n}\theta_j f_j(x) \text{--- для регресии и ранжирования, $Y = \mathbb{R}$};
$$
$$
    g(x, \theta) = \sign\sum\limits_{j=1}^{n}\theta_j f_j(x) \text{--- для классификации, $Y = \{-1, +1\}$}.
$$
\end{frame}

\begin{frame}{Метод обучения}
Метод обучения (learning algorithm) --- это отображение вида
$$
\mu: (X \times Y)^l \rightarrow A,
$$
которое произвольной выборке $S = (x_i, y_i)_{i=1}^{\ell} \subset (X \times Y)^\ell$ ставит в соответсвие некоторый алгоритм $a \in A$.

В задачах обучения по прецедентам всегда есть два этапа:
\begin{itemize}
    \item Этап обучения (training): \\
    метод $\mu$ по выборке $S$ строит алгоритм $a = \mu(S)$.
    \item Этап применеия (testing): \\
    алгоритм $a$ для новых объектов $x \in X$ выбаёт ответы $a(x) \in Y$.

\end{itemize}
\end{frame}

\subsection{Обучение и переобучение}
\begin{frame}{Функционалы качества}
$\mathscr{L}(a, x, y)$ --- функция потерь (loss function) --- величина ошибки алгоритма $a \in A$ на объекте $x \in X$.

{\bf Функции потерь для задач классификации:}
\begin{itemize}
    \item $\mathscr{L}(a, x, y) = [a(x) \ne y(x)]$ --- индикатор ошибки.
\end{itemize}

{\bf Функции потерь для задач регрессии:}
\begin{itemize}
    \item $\mathscr{L}(a, x, y) = |a(x) - y(x)|$ --- абсолютное значение ошибки;
    \item $\mathscr{L}(a, x, y) = \left(a(x) - y(x)\right)^2$ --- квадратичная ошибка.
\end{itemize}

{\it Эмпирический риск} --- функционал качества алгоритма $а$ на $S = \{(x_i, y_i)\}_{i=1}^{\ell}$:
$$
Q(a, S) = \frac{1}{\ell}\sum\limits_{i = 1}^{\ell}\mathscr{L}(a, x_i, y_i).
$$
\end{frame}

\begin{frame}{Сведение задачи обучения к задача оптимизации}
Метод минимизации эмпирического риска:
$$
\mu(S) = \arg\min_{a \in A} Q(a, S)
$$

{\bf Пример:} метод наименьших квадратов ($Y = \mathbb{R}, \mathscr{L}$ квадратична):
$$
\mu(S) = \arg\min_{theta}\sum\limits_{i=1}^{\ell}\left(g(x_i, \theta) - y_i\right)^2.
$$

{\bf Проблемы обобщающей способности:}
\begin{itemize}
    \item найдём ли мы <<закон природы>> или переобучимся (overfitting), т.е. подгоним функцию $g(x_i,\theta)$ под заданные точки?
    \item будет ли $a = \mu(S)$ приближать функцию $y$ на всём $X$?
    \item будет ли $Q(a, S)$ мало на новых данных --- контрольной выборке (validation set) $S' = \{(x_i', y_i')\}_{i=1}^{k}$?
\end{itemize}
\end{frame}

\begin{frame}{Пример переобучения}
Переобучение --- это когда $Q(\mu(S'), S') \gg Q(\mu(S), S)$.

Значения функции потерь для задачи прогноза цены на жилье в Бостоне.
\includegraphics[width=\linewidth]{fig/overfitting.png}
\end{frame}

\begin{frame}{Переобучение --- одна из проблем машинного обучения}
\begin{itemize}
    \item {\bf Из-за чего возниает переобучение?}
    \begin{itemize}
        \item - избыточная сложность пространства параметров $\Theta$, лишние степени свободы в модели $g(x, \theta)$ <<тратятся>> на чрезмерно точную подгонку под обучающую выборку;
        \item переобучение есть всегда, когда есть оптимизация параметров по конечной (заведомо неполной) выборке.
    \end{itemize}
    \item {\bf Как обнаружить переобучение?}
    \begin{itemize}
        \item эмпирически, путём разбиения выборки на обучающую (train) и контрольную (validate, test).
    \end{itemize}
    \item {\bf Избавиться от него нельзя. Как его минимизировать?}
    \begin{itemize}
        \item минимизировать одну из теоретических оценок;
        \item накладывать ограничения на $\theta$ (регуляризация);
        \item минимизировать hold-out, LOO или CV.
    \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{Эмпирические оценки обобщающей способности}
\begin{itemize}
    \item Эмпирический риск на контрольных данных (hold-out):
    $$
        HO(\mu, S, S') = Q(\mu(S), S') \rightarrow \min
    $$
    \item Скользяший контроль (leave one out):
    $$
        LOO(\mu, S) = \frac{1}{\ell}\sum\limits_{i=1}^{\ell}\mathscr{L}(\mu(S\setminus \{x_i\}), y_i) \rightarrow \min
    $$
    \item Кросс-проверка (cross-validation), $S = T_n \sqcup V_n$
    $$
        CV(\mu, S) = \frac{1}{|N|}\sum\limits_{n \in N} Q(\mu(T_n), V_n) \rightarrow \min
    $$
    \item Эмпирическая оценка вероятности переобучения:
    $$
    Q_{\varepsilon}(\mu, S) = \frac{1}{|N|}\sum\limits_{n \in N}\lbrack Q(\mu(T_n), V_n) - Q(\mu(T_n), T_n) \ge \varepsilon \rbrack \rightarrow \min
    $$
\end{itemize}
\end{frame}
% \begin{frame}{Машинное обучение ранжированию <<Интернет-математика 2009>>}

% \end{frame}
\section{Примеры прикладных задач}
\subsection{Задача классификации}
\begin{frame}{Судьба пассажиров Титаника (kaggle.com)}
    $|S| = \ell = 891, |S'| = k = 491, Y = \{0, 1\}$

Таблица признаков
\begin{adjustwidth}{-2.5em}{-2.5em}
\scriptsize
% \tiny
\begin{tabular}{|l|l|l|p{0.15\linewidth}|l|l|l|l|p{0.07\linewidth}|l|l|l|}
\hline
\bf Id & \bf Sur & \bf Cls & \bf Name & \bf Sex & \bf Age & \bf Sb & \bf Pr & \bf Ticket & \bf Fare & \bf Cab & \bf Emb \\
\hline
1 & 0 & 3 & Braund, Mr. Owen Harris & male & 22 & 1 & 0 & A/5 21171 & 7.25 & & S \\
\hline
2 & 1 & 1 & Cumings, Mrs. John Bradley (Florence Briggs Thayer) & female & 38 & 1 & 0 & PC 17599 & 71.2 & C85 & C \\
\hline
3 & 1 & 3 & Heikkinen, Miss. Laina & female & 26 & 0 & 0 & STON. 3101282 & 7.9 & & S \\
\hline
4 & 1 & 1 & Futrelle, Mrs. Jacques Heath (Lily May Peel) & female & 35 & 1 & 0 & 113803 & 53.1 & C123 & S \\
\hline
% 5 & 0 & 3 & Allen, Mr. William Henry & male & 35 & 0 & 0 & 373450 & 8.05 & & S \\
% \hline
\end{tabular}
\end{adjustwidth}

Эмпирический риск на основе индикатора ошибки
$$
Q(a, S') = \frac{1}{k}\sum\limits_{i=1}^{k}\lbrack a(x_i) \ne y(x_i)\rbrack.
$$
\end{frame}
\begin{frame}{Опеределение сорта винограда (UCI)}
$|S| = 177, Y = \{1, 2, 3\}$

\begin{columns}
\column{0.5\linewidth}
Транспонированная таблица признаков
\scriptsize
\begin{tabular}{|l|l|l|l|l|l|l|l|l|l|l|l|l|l|}
\hline
\bf Class & 1 & 1 & 1 \\
\hline
\bf Alcohol & 14.21 & 31.20 & 13.16 \\
\hline
\bf Malic acid & 1.71 & 1.78 & 2.36 \\
\hline
\bf Ash & 2.43 & 2.14 & 2.67 \\
\hline
\bf Alcalinity of ash   & 15.6 & 11.20 & 18.60 \\
\hline
\bf Magnesium & 127 & 100 & 101 \\
\hline
\bf Total phenols & 2.8 & 2.65 & 2.8 \\
\hline
\bf Flavanoids & 3.06 & 2.76 & 3.24 \\
\hline
\bf Nonflav. phenols & 0.28 & 0.26 & 0.30 \\
\hline
\bf Proanthocyanins & 2.29 & 1.28 & 2.81 \\
\hline
\bf Color intensity & 5.64 & 4.38 & 6.68 \\
\hline
\bf Hue & 1.04 & 1.05 & 1.03 \\
\hline
\bf OD280/OD315 & 3.92 & 3.40 & 3.17 \\
\hline
\bf Proline & 1065 & 1050 & 1185 \\
\hline
\end{tabular}
\column{0.5\linewidth}

Пусть $P_{ij}$ --- вероятность принадлежности $i$-го объекта к $j$-му классу и $y_i = (0,\dots, 1, \dots, 0)$ --- бинарный код класса.

\vfill
Перекрёстная энтропия (cross-entropy):
$$
Q(a, S) = \frac{1}{\ell}\sum\limits_{i=1}^{\ell}\sum\limits_{j=1}^{|Y|}y_{ij}\log P_{ij}
$$
\end{columns}
\end{frame}

\subsection{Задача регресии}
\begin{frame}
\end{frame}

\section{Методология машинного обучения}
\subsection{Межотраслевой стандарт CRISP-DM}
\begin{frame}{\large CRISP-DM: Cross Industry Standard Process for Data Mining}
% Cross Industry Standard Process for Data Mining
\begin{center}
\includegraphics[width=0.68\linewidth]{fig/CRISP-DM.png}
\end{center}
\end{frame}
\end{document}